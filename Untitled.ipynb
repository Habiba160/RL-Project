{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee49f42c",
   "metadata": {},
   "source": [
    "# Interactive Reinforcement Learning (RL) Lab\n",
    "\n",
    "This notebook is a **small web-style interactive tool** for learning RL concepts:\n",
    "\n",
    "- **Environments**: GridWorld, FrozenLake, and classic control (CartPole, MountainCar, Breakout, simple custom env via Gym if installed).\n",
    "- **Algorithms**: Policy Evaluation, Policy Iteration, Value Iteration, Monte Carlo (MC), TD(0), n-step TD, SARSA, Q-learning.\n",
    "- **Interactivity**: Use widgets to pick the environment, algorithm, and parameters, then visualize learning.\n",
    "\n",
    "Run the cells from top to bottom, then use the interactive panel near the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871f989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\_ahmed_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\_ahmed_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\_ahmed_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\_ahmed_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\_ahmed_\\appdata\\roaming\\python\\python310\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "   ---------------------------------------- 0.0/914.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/914.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/914.9 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/914.9 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/914.9 kB ? eta -:--:--\n",
      "   --------------------- ---------------- 524.3/914.9 kB 622.7 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 524.3/914.9 kB 622.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 914.9/914.9 kB 708.8 kB/s  0:00:01\n",
      "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 932.9 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.2 MB 987.4 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.2 MB 987.4 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.0/2.2 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 1.1 MB/s  0:00:01\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   ---------------------------------------- 0/3 [widgetsnbextension]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n"
     ]
    }
   ],
   "source": [
    "# Install ipywidgets once so the interactive UI works\n",
    "# (Run this cell, then rerun the imports cell below.)\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d2d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports and global settings ===\n",
    "# We keep dependencies minimal and standard.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# ipywidgets gives us a simple \"web-like\" interactive UI inside the notebook\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import random\n",
    "\n",
    "# Optional: try to import gym / gymnasium for classic control environments\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    try:\n",
    "        import gym  # type: ignore\n",
    "    except ImportError:\n",
    "        gym = None  # Gym is optional; tabular envs will still work\n",
    "\n",
    "# Make plots a bit nicer\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "# Set a random seed for reproducibility (you can change it from the UI if you like)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45ca03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Simple tabular environments: GridWorld and FrozenLake ===\n",
    "# We define very small grid-based environments with a Gym-like API.\n",
    "\n",
    "# Actions: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "ACTION_NAMES = [\"↑\", \"→\", \"↓\", \"←\"]\n",
    "\n",
    "\n",
    "class GridWorldEnv:\n",
    "    \"\"\"Simple deterministic GridWorld.\n",
    "\n",
    "    - The agent moves on an n_rows x n_cols grid.\n",
    "    - Top-left cell (0) is the start, bottom-right cell is the goal.\n",
    "    - Reward = -1 per step until reaching the goal (reward 0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_rows=4, n_cols=4):\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_states = n_rows * n_cols\n",
    "        self.n_actions = 4\n",
    "        self.start_state = 0\n",
    "        self.terminal_state = self.n_states - 1\n",
    "        # Build the transition model P[s][a] = list of (prob, next_state, reward, done)\n",
    "        self.P = self._build_transition_model()\n",
    "        self.state = self.start_state\n",
    "\n",
    "    def _to_pos(self, s):\n",
    "        return divmod(s, self.n_cols)  # (row, col)\n",
    "\n",
    "    def _to_state(self, row, col):\n",
    "        return row * self.n_cols + col\n",
    "\n",
    "    def _build_transition_model(self):\n",
    "        P = {s: {a: [] for a in range(self.n_actions)} for s in range(self.n_states)}\n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                if s == self.terminal_state:\n",
    "                    # Terminal state: stay in place, no reward\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                    continue\n",
    "\n",
    "                row, col = self._to_pos(s)\n",
    "                # Compute next position based on action\n",
    "                if a == 0:  # up\n",
    "                    row2, col2 = max(row - 1, 0), col\n",
    "                elif a == 1:  # right\n",
    "                    row2, col2 = row, min(col + 1, self.n_cols - 1)\n",
    "                elif a == 2:  # down\n",
    "                    row2, col2 = min(row + 1, self.n_rows - 1), col\n",
    "                else:  # left\n",
    "                    row2, col2 = row, max(col - 1, 0)\n",
    "\n",
    "                s2 = self._to_state(row2, col2)\n",
    "                reward = 0.0 if s2 == self.terminal_state else -1.0\n",
    "                done = s2 == self.terminal_state\n",
    "                P[s][a] = [(1.0, s2, reward, done)]\n",
    "        return P\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment and return the starting state.\"\"\"\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return (next_state, reward, done, info).\"\"\"\n",
    "        transitions = self.P[self.state][action]\n",
    "        prob, next_state, reward, done = transitions[0]  # deterministic\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "\n",
    "class FrozenLakeEnv:\n",
    "    \"\"\"Very small FrozenLake-style environment.\n",
    "\n",
    "    - 4x4 grid with start (S), frozen (F), hole (H), and goal (G).\n",
    "    - Stepping into a hole ends the episode with reward -1.\n",
    "    - Reaching the goal ends the episode with reward +1.\n",
    "    - Otherwise reward 0.\n",
    "    - Transitions are deterministic here for simplicity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.desc = [\n",
    "            list(\"SFFF\"),\n",
    "            list(\"FHFH\"),\n",
    "            list(\"FFFH\"),\n",
    "            list(\"HFFG\"),\n",
    "        ]\n",
    "        self.n_rows = len(self.desc)\n",
    "        self.n_cols = len(self.desc[0])\n",
    "        self.n_states = self.n_rows * self.n_cols\n",
    "        self.n_actions = 4\n",
    "        # Locate start and goal\n",
    "        self.start_state = 0\n",
    "        self.goal_state = self.n_states - 1\n",
    "        self.holes = {\n",
    "            self._to_state(r, c)\n",
    "            for r in range(self.n_rows)\n",
    "            for c in range(self.n_cols)\n",
    "            if self.desc[r][c] == \"H\"\n",
    "        }\n",
    "        self.P = self._build_transition_model()\n",
    "        self.state = self.start_state\n",
    "\n",
    "    def _to_pos(self, s):\n",
    "        return divmod(s, self.n_cols)\n",
    "\n",
    "    def _to_state(self, row, col):\n",
    "        return row * self.n_cols + col\n",
    "\n",
    "    def _build_transition_model(self):\n",
    "        P = {s: {a: [] for a in range(self.n_actions)} for s in range(self.n_states)}\n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                row, col = self._to_pos(s)\n",
    "                tile = self.desc[row][col]\n",
    "                if s in self.holes or s == self.goal_state:\n",
    "                    # Terminal states: stay with zero reward\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                    continue\n",
    "\n",
    "                # Move deterministically like GridWorld\n",
    "                if a == 0:  # up\n",
    "                    row2, col2 = max(row - 1, 0), col\n",
    "                elif a == 1:  # right\n",
    "                    row2, col2 = row, min(col + 1, self.n_cols - 1)\n",
    "                elif a == 2:  # down\n",
    "                    row2, col2 = min(row + 1, self.n_rows - 1), col\n",
    "                else:  # left\n",
    "                    row2, col2 = row, max(col - 1, 0)\n",
    "\n",
    "                s2 = self._to_state(row2, col2)\n",
    "                tile2 = self.desc[row2][col2]\n",
    "                if tile2 == \"H\":\n",
    "                    reward, done = -1.0, True\n",
    "                elif tile2 == \"G\":\n",
    "                    reward, done = 1.0, True\n",
    "                else:\n",
    "                    reward, done = 0.0, False\n",
    "\n",
    "                P[s][a] = [(1.0, s2, reward, done)]\n",
    "        return P\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        transitions = self.P[self.state][action]\n",
    "        prob, next_state, reward, done = transitions[0]\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "\n",
    "def plot_grid_values(env, values, policy=None, title=\"Value Function\"):\n",
    "    \"\"\"Visualize a value function (and optional policy arrows) on a grid.\"\"\"\n",
    "    grid = values.reshape(env.n_rows, env.n_cols)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    cmap = colors.LinearSegmentedColormap.from_list(\"vals\", [\"#f7fbff\", \"#08306b\"])\n",
    "    im = ax.imshow(grid, cmap=cmap)\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    for r in range(env.n_rows):\n",
    "        for c in range(env.n_cols):\n",
    "            s = r * env.n_cols + c\n",
    "            text = f\"{grid[r, c]:.1f}\"\n",
    "            ax.text(c, r, text, ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "            if policy is not None:\n",
    "                a = np.argmax(policy[s])\n",
    "                ax.text(\n",
    "                    c,\n",
    "                    r + 0.25,\n",
    "                    ACTION_NAMES[a],\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"red\",\n",
    "                    fontsize=9,\n",
    "                )\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f668c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dynamic Programming (DP) algorithms ===\n",
    "# These algorithms assume we know the transition model env.P[s][a].\n",
    "\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=0.99, theta=1e-4, max_iters=1000):\n",
    "    \"\"\"Iterative Policy Evaluation.\n",
    "\n",
    "    Args:\n",
    "        env: tabular env with P[s][a] list of (prob, next_state, reward, done).\n",
    "        policy: array of shape (n_states, n_actions) with probabilities.\n",
    "        gamma: discount factor.\n",
    "        theta: small threshold for convergence.\n",
    "        max_iters: max number of sweeps over all states.\n",
    "\n",
    "    Returns:\n",
    "        V: estimated value function.\n",
    "        iters: number of sweeps performed.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.n_states)\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        delta = 0.0\n",
    "        # Loop over all states and update V[s]\n",
    "        for s in range(env.n_states):\n",
    "            v = 0.0\n",
    "            # Expected return under the policy\n",
    "            for a, pi_sa in enumerate(policy[s]):\n",
    "                for prob, s2, r, done in env.P[s][a]:\n",
    "                    v += pi_sa * prob * (r + gamma * (0 if done else V[s2]))\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V, it + 1\n",
    "\n",
    "\n",
    "def greedy_policy_improvement(env, V, gamma=0.99):\n",
    "    \"\"\"Improve the policy greedily with respect to V.\n",
    "\n",
    "    Returns a new deterministic policy (one-hot per state).\n",
    "    \"\"\"\n",
    "    policy = np.zeros((env.n_states, env.n_actions))\n",
    "    for s in range(env.n_states):\n",
    "        q_values = np.zeros(env.n_actions)\n",
    "        # Compute Q(s,a) using one-step lookahead\n",
    "        for a in range(env.n_actions):\n",
    "            for prob, s2, r, done in env.P[s][a]:\n",
    "                q_values[a] += prob * (r + gamma * (0 if done else V[s2]))\n",
    "        best_a = np.argmax(q_values)\n",
    "        policy[s, best_a] = 1.0\n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_iteration(env, gamma=0.99, theta=1e-4, max_iters=50):\n",
    "    \"\"\"Classic Policy Iteration (evaluate + improve until stable).\"\"\"\n",
    "    # Start with an equiprobable random policy\n",
    "    policy = np.ones((env.n_states, env.n_actions)) / env.n_actions\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        # Policy Evaluation step\n",
    "        V, _ = policy_evaluation(env, policy, gamma=gamma, theta=theta)\n",
    "        # Policy Improvement step\n",
    "        new_policy = greedy_policy_improvement(env, V, gamma=gamma)\n",
    "        if np.array_equal(new_policy, policy):\n",
    "            # Policy is stable\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy, V\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma=0.99, theta=1e-4, max_iters=1000):\n",
    "    \"\"\"Value Iteration algorithm.\n",
    "\n",
    "    Directly updates V towards the optimal value function.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.n_states)\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(env.n_states):\n",
    "            q_values = np.zeros(env.n_actions)\n",
    "            for a in range(env.n_actions):\n",
    "                for prob, s2, r, done in env.P[s][a]:\n",
    "                    q_values[a] += prob * (r + gamma * (0 if done else V[s2]))\n",
    "            v_new = np.max(q_values)\n",
    "            delta = max(delta, abs(v_new - V[s]))\n",
    "            V[s] = v_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # After convergence, derive a greedy policy\n",
    "    policy = greedy_policy_improvement(env, V, gamma=gamma)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97dc646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model-free tabular algorithms (MC, TD, n-step TD, SARSA, Q-learning) ===\n",
    "# These algorithms learn from experience (episodes) without using env.P.\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    \"\"\"Choose an action using epsilon-greedy exploration from a Q-table.\"\"\"\n",
    "    n_actions = Q.shape[1]\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore: pick a random action\n",
    "        return np.random.randint(n_actions)\n",
    "    # Exploit: pick the best action so far\n",
    "    return int(np.argmax(Q[state]))\n",
    "\n",
    "\n",
    "def monte_carlo_control(env, episodes=300, gamma=0.99, epsilon=0.1, max_steps=100):\n",
    "    \"\"\"Every-visit Monte Carlo control with epsilon-greedy policy.\n",
    "\n",
    "    Returns:\n",
    "        policy: greedy policy derived from Q.\n",
    "        Q: action-value function.\n",
    "        episode_returns: list of returns per episode.\n",
    "    \"\"\"\n",
    "    nS, nA = env.n_states, env.n_actions\n",
    "    Q = np.zeros((nS, nA))\n",
    "    returns_sum = np.zeros((nS, nA))\n",
    "    returns_count = np.zeros((nS, nA))\n",
    "    episode_returns = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        episode = []  # list of (state, action, reward)\n",
    "        s = env.reset()\n",
    "\n",
    "        # Generate one episode using current epsilon-greedy Q\n",
    "        for t in range(max_steps):\n",
    "            a = epsilon_greedy(Q, s, epsilon)\n",
    "            s2, r, done, _ = env.step(a)\n",
    "            episode.append((s, a, r))\n",
    "            s = s2\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Compute returns backwards and update Q\n",
    "        G = 0.0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s_t, a_t, r_t = episode[t]\n",
    "            G = gamma * G + r_t\n",
    "            if (s_t, a_t) not in visited:\n",
    "                visited.add((s_t, a_t))\n",
    "                returns_sum[s_t, a_t] += G\n",
    "                returns_count[s_t, a_t] += 1.0\n",
    "                Q[s_t, a_t] = returns_sum[s_t, a_t] / returns_count[s_t, a_t]\n",
    "\n",
    "        episode_returns.append(G)\n",
    "\n",
    "    # Build a greedy policy from Q\n",
    "    policy = np.zeros_like(Q)\n",
    "    best_actions = np.argmax(Q, axis=1)\n",
    "    policy[np.arange(nS), best_actions] = 1.0\n",
    "    return policy, Q, episode_returns\n",
    "\n",
    "\n",
    "def td0_prediction(env, policy, episodes=300, alpha=0.1, gamma=0.99, max_steps=100):\n",
    "    \"\"\"TD(0) prediction of the state-value function for a fixed policy.\n",
    "\n",
    "    Args:\n",
    "        env: environment with reset() and step().\n",
    "        policy: array (n_states, n_actions) with action probabilities.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.n_states)\n",
    "    episode_returns = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        G = 0.0\n",
    "        for t in range(max_steps):\n",
    "            # Sample action from the given policy\n",
    "            a = np.random.choice(env.n_actions, p=policy[s])\n",
    "            s2, r, done, _ = env.step(a)\n",
    "            G += (gamma ** t) * r\n",
    "\n",
    "            # TD(0) update\n",
    "            target = r + gamma * (0 if done else V[s2])\n",
    "            V[s] += alpha * (target - V[s])\n",
    "\n",
    "            s = s2\n",
    "            if done:\n",
    "                break\n",
    "        episode_returns.append(G)\n",
    "\n",
    "    return V, episode_returns\n",
    "\n",
    "\n",
    "def n_step_td_prediction(env, policy, n=3, episodes=200, alpha=0.1, gamma=0.99, max_steps=100):\n",
    "    \"\"\"n-step TD prediction of the state-value function.\n",
    "\n",
    "    This implementation follows the n-step TD idea from Sutton & Barto,\n",
    "    but with a simple finite-horizon cap (max_steps) to avoid infinite loops\n",
    "    and numerical issues.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.n_states)\n",
    "    episode_returns = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s0 = env.reset()\n",
    "        states = [s0]          # states[0], states[1], ...\n",
    "        rewards = [0.0]        # rewards[1], rewards[2], ... (reward after state t)\n",
    "        T = max_steps          # maximum time index we will consider\n",
    "        t = 0                  # current time step\n",
    "        done = False\n",
    "\n",
    "        while True:\n",
    "            if (t < max_steps) and (not done):\n",
    "                # Choose action from policy and step in the env\n",
    "                s_t = states[-1]\n",
    "                a_t = np.random.choice(env.n_actions, p=policy[s_t])\n",
    "                s_next, r_next, done, _ = env.step(a_t)\n",
    "                states.append(s_next)\n",
    "                rewards.append(r_next)\n",
    "                if done:\n",
    "                    # Episode truly ended at time t+1\n",
    "                    T = t + 1\n",
    "\n",
    "            tau = t - n + 1  # time whose state value we will update\n",
    "            if tau >= 0:\n",
    "                # Compute n-step return G_{tau:tau+n}\n",
    "                G = 0.0\n",
    "                upper = min(tau + n, T)\n",
    "                for i in range(tau + 1, upper + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                if tau + n < T:\n",
    "                    G += (gamma ** n) * V[states[tau + n]]\n",
    "                s_tau = states[tau]\n",
    "                V[s_tau] += alpha * (G - V[s_tau])\n",
    "\n",
    "            if tau >= T - 1:\n",
    "                # All required updates for this episode are done\n",
    "                break\n",
    "            t += 1\n",
    "\n",
    "        # Compute the discounted return of the whole episode for plotting\n",
    "        G_ep = 0.0\n",
    "        for i, r in enumerate(rewards[1:]):\n",
    "            G_ep += (gamma ** i) * r\n",
    "        episode_returns.append(G_ep)\n",
    "\n",
    "    return V, episode_returns\n",
    "\n",
    "\n",
    "def sarsa(env, episodes=300, alpha=0.1, gamma=0.99, epsilon=0.1, max_steps=100):\n",
    "    \"\"\"On-policy SARSA control with epsilon-greedy exploration.\"\"\"\n",
    "    nS, nA = env.n_states, env.n_actions\n",
    "    Q = np.zeros((nS, nA))\n",
    "    episode_returns = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        a = epsilon_greedy(Q, s, epsilon)\n",
    "        G = 0.0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            s2, r, done, _ = env.step(a)\n",
    "            G += (gamma ** t) * r\n",
    "\n",
    "            if done:\n",
    "                # Terminal update\n",
    "                td_target = r\n",
    "                Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "                break\n",
    "\n",
    "            a2 = epsilon_greedy(Q, s2, epsilon)\n",
    "            td_target = r + gamma * Q[s2, a2]\n",
    "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "\n",
    "            s, a = s2, a2\n",
    "\n",
    "        episode_returns.append(G)\n",
    "\n",
    "    # Greedy policy from Q\n",
    "    policy = np.zeros_like(Q)\n",
    "    best_actions = np.argmax(Q, axis=1)\n",
    "    policy[np.arange(nS), best_actions] = 1.0\n",
    "    return policy, Q, episode_returns\n",
    "\n",
    "\n",
    "def q_learning(env, episodes=300, alpha=0.1, gamma=0.99, epsilon=0.1, max_steps=100):\n",
    "    \"\"\"Off-policy Q-learning control with epsilon-greedy behavior policy.\"\"\"\n",
    "    nS, nA = env.n_states, env.n_actions\n",
    "    Q = np.zeros((nS, nA))\n",
    "    episode_returns = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        a = epsilon_greedy(Q, s, epsilon)\n",
    "        G = 0.0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            s2, r, done, _ = env.step(a)\n",
    "            G += (gamma ** t) * r\n",
    "\n",
    "            # Q-learning target uses the greedy value at the next state\n",
    "            best_next = 0.0 if done else np.max(Q[s2])\n",
    "            td_target = r + gamma * best_next\n",
    "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "\n",
    "            s = s2\n",
    "            if done:\n",
    "                break\n",
    "            a = epsilon_greedy(Q, s, epsilon)\n",
    "\n",
    "        episode_returns.append(G)\n",
    "\n",
    "    policy = np.zeros_like(Q)\n",
    "    best_actions = np.argmax(Q, axis=1)\n",
    "    policy[np.arange(nS), best_actions] = 1.0\n",
    "    return policy, Q, episode_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5034b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Custom line environments (Breakout / Gym4ReaL) and environment factory ===\n",
    "# In addition to GridWorld and FrozenLake, we define two very small custom\n",
    "# 1D environments that can be used with all algorithms.\n",
    "\n",
    "\n",
    "class BreakoutEnv:\n",
    "    \"\"\"Simple 1D Breakout-style environment.\n",
    "\n",
    "    - States are positions on a 1D line [0, ..., length-1].\n",
    "    - The agent (paddle) starts in the middle.\n",
    "    - State 0 is a \"hole\" (lose), state length-1 is a \"brick\"/goal (win).\n",
    "    - Actions: 0 = left, 1 = stay, 2 = right.\n",
    "    - Reward +1 at the goal, -1 at the hole, small step penalty elsewhere.\n",
    "\n",
    "    We build a tabular transition model `P` so that DP algorithms can run.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, length=7):\n",
    "        self.length = length\n",
    "        self.n_states = length\n",
    "        self.n_actions = 3  # 0=left, 1=stay, 2=right\n",
    "        self.start_state = length // 2\n",
    "        self.hole_state = 0\n",
    "        self.goal_state = length - 1\n",
    "        self.P = self._build_transition_model()\n",
    "        self.state = self.start_state\n",
    "\n",
    "    def _build_transition_model(self):\n",
    "        P = {s: {a: [] for a in range(self.n_actions)} for s in range(self.n_states)}\n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                # Terminal states: stay in place, zero reward\n",
    "                if s == self.hole_state or s == self.goal_state:\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                    continue\n",
    "\n",
    "                # Compute next position based on action\n",
    "                if a == 0:  # left\n",
    "                    s2 = max(s - 1, 0)\n",
    "                elif a == 2:  # right\n",
    "                    s2 = min(s + 1, self.length - 1)\n",
    "                else:  # stay\n",
    "                    s2 = s\n",
    "\n",
    "                # Rewards\n",
    "                if s2 == self.goal_state:\n",
    "                    reward, done = 1.0, True\n",
    "                elif s2 == self.hole_state:\n",
    "                    reward, done = -1.0, True\n",
    "                else:\n",
    "                    reward, done = -0.1, False  # small step cost\n",
    "\n",
    "                P[s][a] = [(1.0, s2, reward, done)]\n",
    "        return P\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        transitions = self.P[self.state][action]\n",
    "        prob, s2, r, done = transitions[0]\n",
    "        self.state = s2\n",
    "        return s2, r, done, {}\n",
    "\n",
    "\n",
    "class Gym4ReaLEnv:\n",
    "    \"\"\"Simple 1D custom environment (Gym4ReaL-style placeholder).\n",
    "\n",
    "    - Similar line world with a hole and a goal, but without step penalty.\n",
    "    - Rewards: +1 at goal, -1 at hole, 0 otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, length=5):\n",
    "        self.length = length\n",
    "        self.n_states = length\n",
    "        self.n_actions = 3  # 0=left, 1=stay, 2=right\n",
    "        self.start_state = length // 2\n",
    "        self.hole_state = 0\n",
    "        self.goal_state = length - 1\n",
    "        self.P = self._build_transition_model()\n",
    "        self.state = self.start_state\n",
    "\n",
    "    def _build_transition_model(self):\n",
    "        P = {s: {a: [] for a in range(self.n_actions)} for s in range(self.n_states)}\n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                if s == self.hole_state or s == self.goal_state:\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                    continue\n",
    "\n",
    "                if a == 0:  # left\n",
    "                    s2 = max(s - 1, 0)\n",
    "                elif a == 2:  # right\n",
    "                    s2 = min(s + 1, self.length - 1)\n",
    "                else:  # stay\n",
    "                    s2 = s\n",
    "\n",
    "                if s2 == self.goal_state:\n",
    "                    reward, done = 1.0, True\n",
    "                elif s2 == self.hole_state:\n",
    "                    reward, done = -1.0, True\n",
    "                else:\n",
    "                    reward, done = 0.0, False\n",
    "\n",
    "                P[s][a] = [(1.0, s2, reward, done)]\n",
    "        return P\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        transitions = self.P[self.state][action]\n",
    "        prob, s2, r, done = transitions[0]\n",
    "        self.state = s2\n",
    "        return s2, r, done, {}\n",
    "\n",
    "\n",
    "def make_environment(name):\n",
    "    \"\"\"Factory helper: create an environment instance from a string name.\n",
    "\n",
    "    Exposed environments in the UI:\n",
    "    - GridWorld 4x4\n",
    "    - FrozenLake 4x4\n",
    "    - Breakout (custom line)\n",
    "    - Gym4ReaL (custom line)\n",
    "    \"\"\"\n",
    "    if name == \"GridWorld 4x4\":\n",
    "        return GridWorldEnv(4, 4)\n",
    "    if name == \"FrozenLake 4x4\":\n",
    "        return FrozenLakeEnv()\n",
    "    if name == \"Breakout (custom line)\":\n",
    "        return BreakoutEnv(length=7)\n",
    "    if name == \"Gym4ReaL (custom line)\":\n",
    "        return Gym4ReaLEnv(length=5)\n",
    "    raise ValueError(f\"Unknown environment: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c9cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization helpers for training and inference ===\n",
    "\n",
    "\n",
    "def plot_episode_returns(returns, title=\"Episode returns\"):\n",
    "    \"\"\"Plot the return of each episode as a simple learning curve.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    ax.plot(returns, linewidth=1.5)\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Return\")\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_greedy_episode(env, policy, max_steps=50):\n",
    "    \"\"\"Run one greedy episode using a tabular policy.\n",
    "\n",
    "    Returns lists of (states, actions, rewards).\n",
    "    \"\"\"\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        a = int(np.argmax(policy[s]))\n",
    "        s2, r, done, _ = env.step(a)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        s = s2\n",
    "        if done:\n",
    "            states.append(s)\n",
    "            break\n",
    "    return states, actions, rewards\n",
    "\n",
    "\n",
    "def print_episode_trace(states, actions, rewards, env_name=\"Episode\"):\n",
    "    \"\"\"Print a simple text trace of an episode (state, action, reward).\"\"\"\n",
    "    print(f\"=== {env_name} trace ===\")\n",
    "    for t in range(len(actions)):\n",
    "        print(f\"t={t:02d}  s={states[t]}  a={actions[t]}  r={rewards[t]:.2f}\")\n",
    "    print(f\"Terminal state: {states[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26c9dbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a96d5fce0d244f0a01df4ef673806db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Env:', options=('GridWorld 4x4', 'FrozenLake 4x4', 'Breako…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Interactive UI: choose environment, algorithm, and parameters ===\n",
    "# This cell wires everything together using ipywidgets.\n",
    "\n",
    "\n",
    "# Dropdowns for environment and algorithm\n",
    "env_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        \"GridWorld 4x4\",\n",
    "        \"FrozenLake 4x4\",\n",
    "        \"Breakout (custom line)\",\n",
    "        \"Gym4ReaL (custom line)\",\n",
    "    ],\n",
    "    value=\"GridWorld 4x4\",\n",
    "    description=\"Env:\",\n",
    ")\n",
    "\n",
    "alg_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        \"Policy Evaluation\",\n",
    "        \"Policy Iteration\",\n",
    "        \"Value Iteration\",\n",
    "        \"Monte Carlo (MC)\",\n",
    "        \"TD(0)\",\n",
    "        \"n-step TD\",\n",
    "        \"SARSA\",\n",
    "        \"Q-learning\",\n",
    "    ],\n",
    "    value=\"Value Iteration\",\n",
    "    description=\"Algo:\",\n",
    ")\n",
    "\n",
    "# Sliders for main hyperparameters\n",
    "gamma_slider = widgets.FloatSlider(\n",
    "    value=0.99, min=0.0, max=0.999, step=0.01, description=\"gamma\",\n",
    ")\n",
    "alpha_slider = widgets.FloatSlider(\n",
    "    value=0.1, min=0.001, max=1.0, step=0.01, description=\"alpha\",\n",
    ")\n",
    "epsilon_slider = widgets.FloatSlider(\n",
    "    value=0.1, min=0.0, max=1.0, step=0.05, description=\"epsilon\",\n",
    ")\n",
    "\n",
    "episodes_slider = widgets.IntSlider(\n",
    "    value=200, min=10, max=2000, step=10, description=\"episodes\",\n",
    ")\n",
    "max_steps_slider = widgets.IntSlider(\n",
    "    value=50, min=10, max=500, step=10, description=\"max steps\",\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run training\", button_style=\"success\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def on_run_clicked(_):\n",
    "    # Clear previous output and run a new experiment\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        env_name = env_dropdown.value\n",
    "        alg_name = alg_dropdown.value\n",
    "        gamma = gamma_slider.value\n",
    "        alpha = alpha_slider.value\n",
    "        epsilon = epsilon_slider.value\n",
    "        episodes = episodes_slider.value\n",
    "        max_steps = max_steps_slider.value\n",
    "\n",
    "        print(f\"Environment: {env_name}\")\n",
    "        print(f\"Algorithm:   {alg_name}\")\n",
    "        print(f\"gamma={gamma:.3f}, alpha={alpha:.3f}, epsilon={epsilon:.3f}\")\n",
    "        print(f\"episodes={episodes}, max_steps={max_steps}\")\n",
    "        print(\"\\nRunning...\\n\")\n",
    "\n",
    "        # Create environment\n",
    "        try:\n",
    "            env = make_environment(env_name)\n",
    "        except Exception as e:\n",
    "            print(\"Could not create environment:\", e)\n",
    "            return\n",
    "\n",
    "        # Handle each algorithm separately\n",
    "        if alg_name == \"Policy Evaluation\":\n",
    "            # Use a uniform random policy\n",
    "            policy = np.ones((env.n_states, env.n_actions)) / env.n_actions\n",
    "            V, iters = policy_evaluation(env, policy, gamma=gamma)\n",
    "            print(f\"Converged in {iters} sweeps.\")\n",
    "            if hasattr(env, \"n_rows\"):\n",
    "                plot_grid_values(env, V, policy, title=\"Policy Evaluation: V and π\")\n",
    "        elif alg_name == \"Policy Iteration\":\n",
    "            policy, V = policy_iteration(env, gamma=gamma)\n",
    "            if hasattr(env, \"n_rows\"):\n",
    "                plot_grid_values(env, V, policy, title=\"Policy Iteration: optimal V and π\")\n",
    "            states, actions, rewards = run_greedy_episode(env, policy, max_steps=max_steps)\n",
    "            print_episode_trace(states, actions, rewards, env_name=env_name)\n",
    "        elif alg_name == \"Value Iteration\":\n",
    "            policy, V = value_iteration(env, gamma=gamma)\n",
    "            if hasattr(env, \"n_rows\"):\n",
    "                plot_grid_values(env, V, policy, title=\"Value Iteration: optimal V and π\")\n",
    "            states, actions, rewards = run_greedy_episode(env, policy, max_steps=max_steps)\n",
    "            print_episode_trace(states, actions, rewards, env_name=env_name)\n",
    "        elif alg_name == \"Monte Carlo (MC)\":\n",
    "            policy, Q, returns = monte_carlo_control(\n",
    "                env,\n",
    "                episodes=episodes,\n",
    "                gamma=gamma,\n",
    "                epsilon=epsilon,\n",
    "                max_steps=max_steps,\n",
    "            )\n",
    "            plot_episode_returns(returns, title=\"MC control: returns\")\n",
    "            if hasattr(env, \"n_rows\"):\n",
    "                V = np.max(Q, axis=1)\n",
    "                plot_grid_values(env, V, policy, title=\"MC: V and greedy π\")\n",
    "            states, actions, rewards = run_greedy_episode(env, policy, max_steps=max_steps)\n",
    "            print_episode_trace(states, actions, rewards, env_name=env_name)\n",
    "        elif alg_name == \"TD(0)\":\n",
    "            # Use a uniform random behavior policy\n",
    "            policy = np.ones((env.n_states, env.n_actions)) / env.n_actions\n",
    "            V, returns = td0_prediction(\n",
    "                env,\n",
    "                policy,\n",
    "                episodes=episodes,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "                max_steps=max_steps,\n",
    "            )\n",
    "            plot_episode_returns(returns, title=\"TD(0) prediction: returns\")\n",
    "            if hasattr(env, \"n_rows\"):\n",
    "                plot_grid_values(env, V, None, title=\"TD(0): state values under random policy\")\n",
    "        elif alg_name == \"n-step TD\":\n",
    "            policy = np.ones((env.n_states, env.n_actions)) / env.n_actions\n",
    "            V, returns = n_step_td_prediction(\n",
    "                env,\n",
    "                policy,\n",
    "                n=3,\n",
    "                episodes=episodes,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "                max_steps=max_steps,\n",
    "            )\n",
    "            plot_episode_returns(returns, title=\"n-step TD prediction: returns\")\n",
    "            if hasattr(env, \"n_rows\"):\n",
    "                plot_grid_values(env, V, None, title=\"n-step TD: state values under random policy\")\n",
    "        elif alg_name == \"SARSA\":\n",
    "            policy, Q, returns = sarsa(\n",
    "                env,\n",
    "                episodes=episodes,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "                epsilon=epsilon,\n",
    "                max_steps=max_steps,\n",
    "            )\n",
    "            plot_episode_returns(returns, title=\"SARSA control: returns\")\n",
    "            if hasattr(env, \"n_rows\"):\n",
    "                V = np.max(Q, axis=1)\n",
    "                plot_grid_values(env, V, policy, title=\"SARSA: V and greedy π\")\n",
    "            states, actions, rewards = run_greedy_episode(env, policy, max_steps=max_steps)\n",
    "            print_episode_trace(states, actions, rewards, env_name=env_name)\n",
    "        elif alg_name == \"Q-learning\":\n",
    "            policy, Q, returns = q_learning(\n",
    "                env,\n",
    "                episodes=episodes,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "                epsilon=epsilon,\n",
    "                max_steps=max_steps,\n",
    "            )\n",
    "            plot_episode_returns(returns, title=\"Q-learning control: returns\")\n",
    "            if hasattr(env, \"n_rows\"):\n",
    "                V = np.max(Q, axis=1)\n",
    "                plot_grid_values(env, V, policy, title=\"Q-learning: V and greedy π\")\n",
    "            states, actions, rewards = run_greedy_episode(env, policy, max_steps=max_steps)\n",
    "            print_episode_trace(states, actions, rewards, env_name=env_name)\n",
    "        else:\n",
    "            print(\"Unknown algorithm.\")\n",
    "\n",
    "\n",
    "run_button.on_click(on_run_clicked)\n",
    "\n",
    "# Display the UI\n",
    "ui = widgets.VBox(\n",
    "    [\n",
    "        widgets.HBox([env_dropdown, alg_dropdown]),\n",
    "        widgets.HBox([gamma_slider, alpha_slider, epsilon_slider]),\n",
    "        widgets.HBox([episodes_slider, max_steps_slider]),\n",
    "        run_button,\n",
    "        output_area,\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "998eb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Greedy Decode utility (provided in the prompt) ===\n",
    "# This function is independent from RL, but we include it here as a simple\n",
    "# example of sequence decoding with detailed comments.\n",
    "\n",
    "\n",
    "def greedy_decode(log_probs, vocab):\n",
    "    \"\"\"Greedy decoding from log-probabilities over a vocabulary.\n",
    "\n",
    "    Args:\n",
    "        log_probs: NumPy array of shape (Time_Steps, Vocab_Size)\n",
    "                   containing log-probabilities at each time step.\n",
    "        vocab: list of vocabulary symbols; vocab[i] is the symbol for index i.\n",
    "\n",
    "    Returns:\n",
    "        final_output: list of decoded symbols after collapsing repeats\n",
    "                      and removing 'blank'.\n",
    "    \"\"\"\n",
    "    # 1. Get the index of the max probability at each time step\n",
    "    # axis=1 means we look across the vocabulary dimension\n",
    "    best_indices = np.argmax(log_probs, axis=1)\n",
    "\n",
    "    # 2. Convert indices to vocabulary symbols\n",
    "    raw_path = [vocab[i] for i in best_indices]\n",
    "\n",
    "    # 3. Collapse the path (remove repeats and blanks)\n",
    "    final_output = []\n",
    "    prev_char = None\n",
    "\n",
    "    for char in raw_path:\n",
    "        # Only add if it's not a repeat of the previous one\n",
    "        if char != prev_char:\n",
    "            # Skip the special 'blank' symbol if present\n",
    "            if char != \"blank\":\n",
    "                final_output.append(char)\n",
    "        prev_char = char\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9731f05",
   "metadata": {},
   "source": [
    "# Final Project Report: Interactive RL Learning Tool (GridWorld & FrozenLake)\n",
    "\n",
    "## 1. Objective\n",
    "\n",
    "This project implements a **small interactive RL learning tool** inside a Jupyter notebook.  \n",
    "The goal is to help users understand key **reinforcement learning algorithms** on simple **tabular environments** with:\n",
    "\n",
    "- Multiple algorithms (DP + model-free).\n",
    "- Adjustable hyperparameters via sliders.\n",
    "- Visualizations of value functions, learning curves, and example trajectories.\n",
    "\n",
    "The tool behaves like a lightweight web app using `ipywidgets` for interactivity.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Environments\n",
    "\n",
    "The interactive UI focuses on two core **tabular MDPs**:\n",
    "\n",
    "- **GridWorld 4x4 (`GridWorldEnv`)**  \n",
    "  - 4×4 deterministic grid.  \n",
    "  - Start at top-left, goal at bottom-right.  \n",
    "  - Reward = −1 per step, 0 at the terminal goal.  \n",
    "  - Used heavily for visualizing value functions and policies (arrows on the grid).\n",
    "\n",
    "- **FrozenLake 4x4 (`FrozenLakeEnv`)**  \n",
    "  - Deterministic version of FrozenLake with tiles: Start (S), Frozen (F), Hole (H), Goal (G).  \n",
    "  - Stepping into a Hole: reward −1, terminal.  \n",
    "  - Reaching the Goal: reward +1, terminal.  \n",
    "  - Other transitions: reward 0.  \n",
    "  - Provides a slightly more complex environment with both positive and negative terminal rewards.\n",
    "\n",
    "Additional environment classes (e.g., simple custom line world, discretized CartPole / MountainCar) are defined in the code but **not exposed in the current UI**, so the user experience remains focused on the first two environments.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Algorithms\n",
    "\n",
    "All requested algorithms are implemented in a **clear, tabular form** with comments explaining each step.\n",
    "\n",
    "### 3.1 Dynamic Programming (requires known model `env.P`)\n",
    "\n",
    "- **Policy Evaluation (`policy_evaluation`)**  \n",
    "  - Inputs: known transition model and a fixed policy π(s,a).  \n",
    "  - Iteratively updates `V(s)` using the Bellman expectation equation until the maximum change is below a threshold `θ`.  \n",
    "  - Demonstrates how a given policy induces a value function.\n",
    "\n",
    "- **Policy Iteration (`policy_iteration`)**  \n",
    "  - Alternates between:  \n",
    "    1. **Policy Evaluation** to compute `V^π`.  \n",
    "    2. **Policy Improvement** using `greedy_policy_improvement` to make π greedy w.r.t. `V^π`.  \n",
    "  - Repeats until the policy stops changing, yielding an optimal policy and value function.\n",
    "\n",
    "- **Value Iteration (`value_iteration`)**  \n",
    "  - Updates `V(s)` directly using the Bellman **optimality** equation:  \n",
    "    `V(s) ← max_a Σ_s' p(s'|s,a)[r + γ V(s')]`.  \n",
    "  - After convergence, a greedy policy is extracted from the resulting `V`.  \n",
    "  - Typically converges quickly on small tabular problems like GridWorld and FrozenLake.\n",
    "\n",
    "### 3.2 Model-Free Methods (learn from interaction only)\n",
    "\n",
    "- **Monte Carlo Control (`monte_carlo_control`)**  \n",
    "  - Every-visit MC algorithm with **ε-greedy** exploration.  \n",
    "  - Generates full episodes, computes returns, and averages them to estimate `Q(s,a)`.  \n",
    "  - Greedy policy is obtained by taking `argmax_a Q(s,a)` in each state.\n",
    "\n",
    "- **TD(0) Prediction (`td0_prediction`)**  \n",
    "  - One-step Temporal Difference method for estimating `V(s)` under a fixed policy.  \n",
    "  - Updates `V(s)` towards `r + γ V(s')` after each step, combining bootstrapping and sampled transitions.\n",
    "\n",
    "- **n-step TD Prediction (`n_step_td_prediction`)**  \n",
    "  - Generalization of TD(0) using n-step returns `G_{t:t+n}`.  \n",
    "  - Implementation includes a simple **finite-horizon cap (`max_steps`)** to avoid infinite loops and numerical issues.  \n",
    "  - Compatible with both GridWorld and FrozenLake.\n",
    "\n",
    "- **SARSA Control (`sarsa`)**  \n",
    "  - On-policy TD control algorithm.  \n",
    "  - Uses ε-greedy behavior and updates `Q(s,a)` toward `r + γ Q(s',a')`, where `a'` is the next action taken by the same policy.  \n",
    "  - Produces a greedy tabular policy.\n",
    "\n",
    "- **Q-learning Control (`q_learning`)**  \n",
    "  - Off-policy TD control algorithm.  \n",
    "  - Updates toward `r + γ max_{a'} Q(s',a')` while acting ε-greedily.  \n",
    "  - Approximates the optimal action-value function `Q*`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Interactive UI and Parameter Adjustment\n",
    "\n",
    "The notebook uses **`ipywidgets`** to build a small web-style interface:\n",
    "\n",
    "- **Environment selector**: dropdown with `GridWorld 4x4` and `FrozenLake 4x4`.\n",
    "- **Algorithm selector**: dropdown with\n",
    "  - Policy Evaluation, Policy Iteration, Value Iteration, Monte Carlo (MC), TD(0), n-step TD, SARSA, Q-learning.\n",
    "- **Hyperparameter sliders**:\n",
    "  - Discount factor `γ` (gamma).  \n",
    "  - Learning rate `α` (alpha) for TD-based algorithms.  \n",
    "  - Exploration rate `ε` (epsilon) for MC, SARSA, and Q-learning.  \n",
    "  - Number of episodes.  \n",
    "  - Maximum steps per episode.\n",
    "\n",
    "When the user clicks **\"Run training\"**, the tool:\n",
    "\n",
    "1. Instantiates the selected environment.  \n",
    "2. Runs the chosen algorithm with the current slider settings.  \n",
    "3. Displays learning curves, value function plots, and a sample greedy episode trace (where applicable).\n",
    "\n",
    "This setup lets the user **see in real time** how changing `γ`, `α`, `ε`, and the number of episodes affects convergence and learned behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Visualization of Training and Agent Behavior\n",
    "\n",
    "The tool includes several visualization components:\n",
    "\n",
    "- **Value Function and Policy Plots** (Grid-based envs)  \n",
    "  - `plot_grid_values` shows `V(s)` as a color-coded grid.  \n",
    "  - Overlays arrows indicating the greedy action in each state (policy visualization).\n",
    "\n",
    "- **Learning Curves (Episode Returns)**  \n",
    "  - `plot_episode_returns` plots the return per episode for MC, TD(0), n-step TD, SARSA, and Q-learning.  \n",
    "  - Helps visualize convergence, stability, and the impact of hyperparameters.\n",
    "\n",
    "- **Episode Traces**  \n",
    "  - `run_greedy_episode` and `print_episode_trace` generate and print a greedy trajectory:  \n",
    "    time step, state, action, and reward.  \n",
    "  - Useful for understanding actual agent behavior in GridWorld and FrozenLake.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Usage Instructions and Deliverables\n",
    "\n",
    "1. **Run all cells** in the notebook from top to bottom (including the `ipywidgets` installation cell once).  \n",
    "2. In the UI cell (`display(ui)`):\n",
    "   - Choose `Env` = `GridWorld 4x4` or `FrozenLake 4x4`.  \n",
    "   - Select an algorithm.  \n",
    "   - Adjust the sliders for `gamma`, `alpha`, `epsilon`, `episodes`, and `max steps`.  \n",
    "   - Click **\"Run training\"**.\n",
    "3. Inspect the generated plots and episode traces to analyze learning.\n",
    "\n",
    "For the final submission:\n",
    "\n",
    "- The **notebook itself** serves as the main artifact (interactive RL tool).  \n",
    "- You can push it to **GitHub** and reference the repository link as the required deliverable.  \n",
    "- This final report cell documents the implemented environments, algorithms, hyperparameters, UI, and visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c91bb",
   "metadata": {},
   "source": [
    "# Project Report: Interactive Reinforcement Learning (RL) Learning Tool\n",
    "\n",
    "## 1. Objective\n",
    "\n",
    "The goal of this project is to build a **web-style interactive tool** (implemented as a Jupyter notebook with widgets) that helps users understand and experiment with fundamental **reinforcement learning (RL)** algorithms and environments. The tool targets students and practitioners who want an **intuitive, visual, and configurable** way to explore RL.\n",
    "\n",
    "The tool allows users to:\n",
    "\n",
    "- Select among several environments (tabular and discretized classic control).\n",
    "- Choose a learning algorithm.\n",
    "- Adjust key hyperparameters.\n",
    "- Observe training curves, value functions, policies, and example episodes.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Implemented Environments\n",
    "\n",
    "All environments expose a **simple, Gym-like interface** with `reset()` and `step(action)` methods, and tabular state indices for compatibility with classic RL algorithms.\n",
    "\n",
    "- **GridWorld 4x4 (`GridWorldEnv`)**  \n",
    "  - Deterministic 4×4 grid.  \n",
    "  - Start at top-left cell (state 0), goal at bottom-right.  \n",
    "  - Reward = −1 per step, 0 at the goal.  \n",
    "  - Used mainly for **Dynamic Programming (DP)** algorithms and visualization of values/policies.\n",
    "\n",
    "- **FrozenLake 4x4 (`FrozenLakeEnv`)**  \n",
    "  - Deterministic version of FrozenLake with tiles: Start (S), Frozen (F), Hole (H), Goal (G).  \n",
    "  - Stepping into a hole: reward −1 and terminal.  \n",
    "  - Reaching the goal: reward +1 and terminal.  \n",
    "  - Other transitions: reward 0.  \n",
    "  - Suitable for both DP and model-free methods.\n",
    "\n",
    "- **Simple Custom 1D Env (`SimpleCustomEnv`)**  \n",
    "  - 1D line world of `N` cells (default 7).  \n",
    "  - Agent starts in the middle.  \n",
    "  - Leftmost cell is a \"hole\" (reward −1, terminal).  \n",
    "  - Rightmost cell is a goal (reward +1, terminal).  \n",
    "  - Other transitions have reward 0.  \n",
    "  - Serves as a **minimal custom environment** (Breakout / Gym4ReaL-style placeholder).\n",
    "\n",
    "- **CartPole (discretized) (`DiscretizedCartPoleEnv`)**  \n",
    "  - Wraps `CartPole-v1` from Gym / Gymnasium.  \n",
    "  - Continuous state `(x, x_dot, theta, theta_dot)` is **discretized** into bins along each dimension.  \n",
    "  - The resulting discrete state index is used with tabular methods (e.g., SARSA, Q-learning).  \n",
    "  - Demonstrates applying tabular RL to a classic control problem.\n",
    "\n",
    "- **MountainCar (discretized) (`DiscretizedMountainCarEnv`)**  \n",
    "  - Wraps `MountainCar-v0`.  \n",
    "  - Continuous state `(position, velocity)` is discretized into a grid of bins.  \n",
    "  - Allows application of tabular control methods to a sparse-reward control problem.\n",
    "\n",
    "> Note: CartPole and MountainCar require `gym` / `gymnasium`. If unavailable, the tabular environments still function.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Implemented Algorithms\n",
    "\n",
    "All algorithms are implemented in a **simple, tabular form** with clear comments and minimal code.\n",
    "\n",
    "### 3.1 Dynamic Programming (DP)\n",
    "\n",
    "- **Policy Evaluation (`policy_evaluation`)**  \n",
    "  - Input: known transition model `env.P`, fixed policy `π(s,a)`.  \n",
    "  - Iteratively updates `V(s)` using the **Bellman expectation equation** until convergence (`θ` threshold).  \n",
    "  - Demonstrates how a policy induces a value function.\n",
    "\n",
    "- **Policy Iteration (`policy_iteration`)**  \n",
    "  - Alternates between:  \n",
    "    1. **Policy Evaluation** (estimate `V^π`).  \n",
    "    2. **Greedy Policy Improvement** (derive new policy π' greedy w.r.t. `V^π`).  \n",
    "  - Repeats until the policy is stable.  \n",
    "  - Produces an optimal policy `π*` and value function `V*` for the environment.\n",
    "\n",
    "- **Value Iteration (`value_iteration`)**  \n",
    "  - Directly updates `V(s)` using the **Bellman optimality equation** (max over actions).  \n",
    "  - After convergence, derives a greedy policy from `V`.  \n",
    "  - Typically converges faster than full policy iteration in small problems.\n",
    "\n",
    "### 3.2 Model-Free Methods\n",
    "\n",
    "- **Monte Carlo Control (`monte_carlo_control`)**  \n",
    "  - Every-visit Monte Carlo with **ε-greedy** exploration.  \n",
    "  - Learns action-value function `Q(s,a)` from sampled **episodic returns**.  \n",
    "  - Produces a greedy policy as `π(s) = argmax_a Q(s,a)`.\n",
    "\n",
    "- **TD(0) Prediction (`td0_prediction`)**  \n",
    "  - One-step **Temporal Difference** method for learning `V(s)` under a fixed policy.  \n",
    "  - Updates towards `r + γ V(s')` at each step, combining bootstrapping and sampling.\n",
    "\n",
    "- **n-step TD Prediction (`n_step_td_prediction`)**  \n",
    "  - Generalization of TD(0) using **n-step returns** `G_{t:t+n}`.  \n",
    "  - Balances between Monte Carlo (long returns) and TD(0) (one-step returns).\n",
    "\n",
    "- **SARSA Control (`sarsa`)**  \n",
    "  - On-policy TD control algorithm.  \n",
    "  - Uses **ε-greedy** behavior policy and updates towards `r + γ Q(s', a')`.  \n",
    "  - Learns `Q(s,a)` and a corresponding greedy policy.\n",
    "\n",
    "- **Q-learning Control (`q_learning`)**  \n",
    "  - Off-policy TD control algorithm.  \n",
    "  - Updates towards `r + γ max_{a'} Q(s', a')` while still acting ε-greedily.  \n",
    "  - Learns an approximation to the **optimal** action-value function `Q*`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Parameter Adjustment and Interactive UI\n",
    "\n",
    "The notebook uses **`ipywidgets`** to provide a simple web-like interface:\n",
    "\n",
    "- **Environment selection**: dropdown (`GridWorld`, `FrozenLake`, `Simple Custom`, `CartPole`, `MountainCar`).\n",
    "- **Algorithm selection**: dropdown (Policy Evaluation, Policy Iteration, Value Iteration, MC, TD(0), n-step TD, SARSA, Q-learning).\n",
    "- **Adjustable parameters** via sliders:\n",
    "  - Discount factor `γ` (gamma).  \n",
    "  - Learning rate `α` (alpha) for TD-style methods.  \n",
    "  - Exploration rate `ε` (epsilon) for MC and control algorithms.  \n",
    "  - Number of episodes.  \n",
    "  - Maximum steps per episode.\n",
    "\n",
    "With one click on **\"Run training\"**, the tool:\n",
    "\n",
    "1. Instantiates the chosen environment.  \n",
    "2. Runs the selected algorithm with the specified parameters.  \n",
    "3. Displays training curves, value functions, policies, and a sample episode trace.\n",
    "\n",
    "This design makes it easy to **experiment in real time** with how `γ`, `α`, `ε`, and training time affect convergence and behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Visualization Techniques\n",
    "\n",
    "To support learning and intuition, the tool provides multiple visualizations:\n",
    "\n",
    "- **Value Function Heatmaps (Grid Environments)**  \n",
    "  - For GridWorld and FrozenLake, `plot_grid_values` shows `V(s)` as a colored grid.  \n",
    "  - Red arrows overlay the **greedy policy** (argmax over action probabilities or Q-values).\n",
    "\n",
    "- **Learning Curves (Episode Returns)**  \n",
    "  - For MC, TD(0), n-step TD, SARSA, and Q-learning, `plot_episode_returns` shows the return per episode.  \n",
    "  - This illustrates convergence trends and the impact of parameter changes.\n",
    "\n",
    "- **Episode Traces**  \n",
    "  - For control algorithms, `run_greedy_episode` + `print_episode_trace` show an example greedy trajectory:  \n",
    "    - Time step, state index, action, and reward.  \n",
    "  - Helps interpret the learned behavior, especially on the simple custom environment.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Usage Instructions\n",
    "\n",
    "1. Install dependencies (at minimum: `numpy`, `matplotlib`, `ipywidgets`; optionally `gym` / `gymnasium`).  \n",
    "2. Open the notebook and **run all cells** from top to bottom.  \n",
    "3. In the interactive UI cell:\n",
    "   - Choose an environment and algorithm.  \n",
    "   - Adjust `γ`, `α`, `ε`, episodes, and max steps as desired.  \n",
    "   - Click **\"Run training\"** to visualize the results.\n",
    "\n",
    "The notebook therefore acts as a **self-contained, interactive RL lab**, satisfying the requirements on environments, algorithms, parameter adjustment, visualization, and user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34620e35",
   "metadata": {},
   "source": [
    "# Updated Final Report: RL Tool with Custom Breakout / Gym4ReaL Environments\n",
    "\n",
    "## 1. Objective\n",
    "\n",
    "This notebook implements an interactive **Reinforcement Learning (RL) learning tool**.  \n",
    "Users can explore several tabular environments and core RL algorithms, adjust parameters with sliders, and visualize value functions, learning curves, and sample trajectories.\n",
    "\n",
    "The tool behaves like a small web app using `ipywidgets` inside Jupyter / Colab.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Environments\n",
    "\n",
    "All environments are **tabular** (finite states and actions) and expose a Gym-like API (`reset`, `step`).\n",
    "\n",
    "- **GridWorld 4x4 (`GridWorldEnv`)**  \n",
    "  - Deterministic 4×4 grid.  \n",
    "  - Start: top-left, Goal: bottom-right.  \n",
    "  - Reward −1 per step, 0 at goal.  \n",
    "  - Has full transition model `P` for DP algorithms.\n",
    "\n",
    "- **FrozenLake 4x4 (`FrozenLakeEnv`)**  \n",
    "  - Deterministic version of FrozenLake with tiles S/F/H/G.  \n",
    "  - Hole: reward −1, terminal.  \n",
    "  - Goal: reward +1, terminal.  \n",
    "  - Other moves: reward 0.  \n",
    "  - Also exposes `P` for DP and model-free methods.\n",
    "\n",
    "- **Breakout (custom line) (`BreakoutEnv`)**  \n",
    "  - 1D line of cells, agent starts in the middle.  \n",
    "  - Left end is a \"hole\" (lose), right end is a \"brick\"/goal (win).  \n",
    "  - Actions: left, stay, right.  \n",
    "  - Rewards: +1 at goal, −1 at hole, small negative step cost elsewhere.  \n",
    "  - Full tabular transition model `P` + `reset`/`step` → works with **all algorithms**.\n",
    "\n",
    "- **Gym4ReaL (custom line) (`Gym4ReaLEnv`)**  \n",
    "  - Another 1D line environment, similar to Breakout but with no step cost.  \n",
    "  - Rewards: +1 at goal, −1 at hole, 0 otherwise.  \n",
    "  - Also provides `P`, so it is compatible with DP and model-free algorithms.\n",
    "\n",
    "These custom line environments satisfy the requirement of having **custom environments (Breakout / Gym4ReaL)** and are simple enough to understand easily.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Algorithms\n",
    "\n",
    "All required algorithms are implemented in a clear tabular form:\n",
    "\n",
    "- **Policy Evaluation** (`policy_evaluation`)  \n",
    "  - Iteratively computes `V(s)` for a fixed policy using the Bellman expectation equation.\n",
    "\n",
    "- **Policy Iteration** (`policy_iteration`)  \n",
    "  - Alternates between policy evaluation and greedy policy improvement until the policy is stable.\n",
    "\n",
    "- **Value Iteration** (`value_iteration`)  \n",
    "  - Directly updates `V(s)` using the Bellman optimality equation and then derives a greedy policy.\n",
    "\n",
    "- **Monte Carlo Control (MC)** (`monte_carlo_control`)  \n",
    "  - Every-visit MC with ε-greedy exploration; learns `Q(s,a)` and a greedy policy.\n",
    "\n",
    "- **TD(0) Prediction** (`td0_prediction`)  \n",
    "  - One-step temporal difference method to estimate `V(s)` for a fixed policy.\n",
    "\n",
    "- **n-step TD Prediction** (`n_step_td_prediction`)  \n",
    "  - Generalizes TD(0) to n-step returns, with a finite horizon (`max_steps`) to avoid numerical issues.\n",
    "\n",
    "- **SARSA Control** (`sarsa`)  \n",
    "  - On-policy TD control algorithm that updates `Q(s,a)` toward `r + γ Q(s', a')`.\n",
    "\n",
    "- **Q-learning Control** (`q_learning`)  \n",
    "  - Off-policy TD control algorithm that updates toward `r + γ max_{a'} Q(s', a')`.\n",
    "\n",
    "These algorithms work on **all four** tabular environments thanks to the consistent API and, for DP algorithms, the availability of `P`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Interactive UI and Parameter Adjustment\n",
    "\n",
    "The main interface is built with `ipywidgets` and includes:\n",
    "\n",
    "- **Environment selector**: \n",
    "  - `GridWorld 4x4`, `FrozenLake 4x4`, `Breakout (custom line)`, `Gym4ReaL (custom line)`.\n",
    "- **Algorithm selector**: \n",
    "  - Policy Evaluation, Policy Iteration, Value Iteration, MC, TD(0), n-step TD, SARSA, Q-learning.\n",
    "- **Hyperparameter sliders**:\n",
    "  - Discount factor `gamma`.  \n",
    "  - Learning rate `alpha`.  \n",
    "  - Exploration rate `epsilon`.  \n",
    "  - Number of episodes.  \n",
    "  - Maximum steps per episode.\n",
    "\n",
    "Pressing **\"Run training\"** runs the selected algorithm on the chosen environment with the current settings and displays the results.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Visualization of Training and Behavior\n",
    "\n",
    "- **Value function + policy plots** for grid environments using `plot_grid_values`.  \n",
    "- **Episode return curves** with `plot_episode_returns` for MC/TD/SARSA/Q-learning.  \n",
    "- **Episode traces** printed via `run_greedy_episode` + `print_episode_trace`, showing state, action, and reward at each time step.\n",
    "\n",
    "These visualizations help understand both the **training process** and the **behavior** of the learned policies in different environments.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Usage & Deliverables\n",
    "\n",
    "1. Install dependencies (`numpy`, `matplotlib`, `ipywidgets`; Jupyter/Colab already included).  \n",
    "2. Run all cells from top to bottom.  \n",
    "3. Use the interactive UI cell (`display(ui)`) to:\n",
    "   - Select environment and algorithm,  \n",
    "   - Adjust parameters,  \n",
    "   - Run training and inspect plots / traces.\n",
    "\n",
    "For submission, push this notebook to **GitHub** as the main artifact of the project and reference the repository link.  \n",
    "This updated report describes all implemented environments, algorithms, the interactive UI, and visualization choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14a9a3",
   "metadata": {},
   "source": [
    "# Final Project Report – Interactive RL Web Tool Inside This Notebook\n",
    "\n",
    "This notebook implements a **fully interactive, web‑style tool** for learning Reinforcement Learning (RL) using only standard Python + `ipywidgets`. No extra `.py` files are required.\n",
    "\n",
    "## Implemented Environments\n",
    "\n",
    "All environments are **tabular** and expose a Gym‑like API: `reset()`, `step(action)`, and attributes `n_states`, `n_actions`.\n",
    "\n",
    "- **GridWorld 4x4**  \n",
    "  Small deterministic grid; start in the top‑left, goal in the bottom‑right.  \n",
    "  Reward = −1 per step, 0 at the terminal state.\n",
    "\n",
    "- **FrozenLake 4x4**  \n",
    "  Deterministic version of the classic FrozenLake.  \n",
    "  Tiles: Start (S), Frozen (F), Hole (H), Goal (G).  \n",
    "  Rewards: −1 in a hole, +1 at the goal, 0 otherwise.\n",
    "\n",
    "- **Breakout (custom line)**  \n",
    "  1D line of states with a **hole** at the left end and a **goal brick** at the right end.  \n",
    "  Actions: left, stay, right.  \n",
    "  Rewards: +1 at goal, −1 at hole, small step cost in between.\n",
    "\n",
    "- **Gym4ReaL (custom line)**  \n",
    "  Another simple 1D line world with hole + goal but **no step penalty**.  \n",
    "  Rewards: +1 at goal, −1 at hole, 0 otherwise.\n",
    "\n",
    "These four environments are exposed in the **Env** dropdown of the UI via the `make_environment` factory.\n",
    "\n",
    "## Implemented Algorithms\n",
    "\n",
    "All algorithms are implemented in **simple, commented, tabular form**:\n",
    "\n",
    "- **Policy Evaluation**  \n",
    "  Iterative policy evaluation using the full transition model `env.P[s][a]` and a fixed policy.\n",
    "\n",
    "- **Policy Iteration**  \n",
    "  Alternates **policy evaluation** and **greedy policy improvement** until the policy is stable.\n",
    "\n",
    "- **Value Iteration**  \n",
    "  Directly updates the value function toward the optimal value, then extracts a greedy policy.\n",
    "\n",
    "- **Monte Carlo (MC) Control**  \n",
    "  Every‑visit Monte Carlo with epsilon‑greedy exploration.  \n",
    "  Learns an action‑value function `Q` and a greedy policy.\n",
    "\n",
    "- **TD(0) Prediction**  \n",
    "  Temporal‑Difference prediction of the state‑value function under a fixed policy.\n",
    "\n",
    "- **n‑step TD Prediction**  \n",
    "  n‑step TD with a finite horizon (`max_steps`) to avoid overflow / infinite loops.  \n",
    "  This fixes the earlier `OverflowError`.\n",
    "\n",
    "- **SARSA (On‑policy)**  \n",
    "  On‑policy control: updates `Q` using the value of the **next action actually taken**.\n",
    "\n",
    "- **Q‑learning (Off‑policy)**  \n",
    "  Off‑policy control: updates `Q` toward the **max over next‑state actions**.\n",
    "\n",
    "The algorithms are used by the UI according to the selected **Algo** in the dropdown.\n",
    "\n",
    "## Parameter Adjustment (Hyperparameters)\n",
    "\n",
    "The interactive panel exposes the main RL parameters:\n",
    "\n",
    "- **gamma** – discount factor (0 to 0.999).  \n",
    "- **alpha** – learning rate for TD / SARSA / Q‑learning (0.001 to 1.0).  \n",
    "- **epsilon** – exploration rate for epsilon‑greedy policies (0 to 1).  \n",
    "- **episodes** – number of training episodes for model‑free methods.  \n",
    "- **max steps** – maximum steps per episode (also used as a finite horizon for n‑step TD).\n",
    "\n",
    "Changing these sliders and clicking **Run training** immediately re‑runs the selected algorithm with the new parameters.\n",
    "\n",
    "## Visualization of Training and Inference\n",
    "\n",
    "The notebook provides several visualizations to understand learning:\n",
    "\n",
    "- **Value function heatmaps (grid environments)**  \n",
    "  For GridWorld and FrozenLake, `plot_grid_values` shows:  \n",
    "  - A heatmap of state values `V(s)`.  \n",
    "  - Optional policy arrows (up/right/down/left) when a policy is available.\n",
    "\n",
    "- **Learning curves**  \n",
    "  `plot_episode_returns` plots the **return per episode** for Monte Carlo, TD(0), n‑step TD, SARSA, and Q‑learning.\n",
    "\n",
    "- **Episode traces**  \n",
    "  `run_greedy_episode` + `print_episode_trace` show a text trace of:  \n",
    "  state, action, and reward at each time step under the learned greedy policy.\n",
    "\n",
    "These visualizations together illustrate both **training dynamics** (convergence of values / returns) and **agent behavior** (how the agent moves in the environment).\n",
    "\n",
    "## Interactive User Interface (\"Web‑like\" Tool)\n",
    "\n",
    "The cell titled **\"Interactive UI: choose environment, algorithm, and parameters\"** builds a simple web‑style panel using `ipywidgets`:\n",
    "\n",
    "- **Dropdowns** to select:  \n",
    "  - Environment: GridWorld / FrozenLake / Breakout / Gym4ReaL.  \n",
    "  - Algorithm: Policy Evaluation, Policy Iteration, Value Iteration, MC, TD(0), n‑step TD, SARSA, Q‑learning.\n",
    "- **Sliders** to control `gamma`, `alpha`, `epsilon`, `episodes`, and `max steps`.\n",
    "- A **Run training** button that executes the chosen algorithm and displays:\n",
    "  - Plots (value functions or learning curves).  \n",
    "  - A text episode trace when a greedy policy is available.\n",
    "\n",
    "This behaves like a **small web app inside the notebook**: you can change settings and re‑run experiments without editing any code.\n",
    "\n",
    "## How to Use This Notebook as the Web Tool\n",
    "\n",
    "1. **Install ipywidgets (once):** run the `!pip install ipywidgets` cell at the top if widgets do not appear.  \n",
    "2. **Run all setup cells:** run the imports, environment definitions, algorithm definitions, and helper functions in order.  \n",
    "3. **Scroll to the UI cell:** look for the cell that displays the `Env` and `Algo` dropdowns and the **Run training** button.  \n",
    "4. **Pick an environment and algorithm**, adjust parameters, then click **Run training**.  \n",
    "5. **Inspect the outputs:** plots and text traces will appear directly under the UI.\n",
    "\n",
    "Everything required for the project (environments, algorithms, parameter control, and visualization) is contained **entirely in this `Untitled.ipynb` file**, so no extra Python modules or configuration files are needed for the interactive RL lab.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
